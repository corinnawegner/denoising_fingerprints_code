{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning of the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will find the best parameters for our dense layer model architecture by trying some values out. For grid search, a gpu would have been necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "layers = keras.layers\n",
    "\n",
    "#importing the data\n",
    "gen = np.loadtxt(\"qcs_gen10000.txt\", delimiter=\",\")\n",
    "\n",
    "#splitting data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(gen, test_size=0.2, random_state=42, shuffle=True, stratify=None)\n",
    "\n",
    "mean_train = np.mean(train, axis=1, dtype=np.float64)\n",
    "mean_test = np.mean(test, axis=1, dtype=np.float64)\n",
    "stddev_train = np.std(train, axis=1, dtype=np.float64)\n",
    "stddev_test = np.std(test, axis=1, dtype=np.float64)\n",
    "\n",
    "#standard scaling the training data\n",
    "def standard_scale(X, m, s):\n",
    "    return (X-m)/s\n",
    "\n",
    "def rescale(X, m, s):\n",
    "    return X*s+m\n",
    "\n",
    "train = np.array([standard_scale(train[i], mean_train[i], stddev_train[i]) for i in range(len(train))])\n",
    "test_scaled = np.array([standard_scale(test[i], mean_test[i], stddev_test[i]) for i in range(len(test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ provides an explaination \n",
    "\n",
    "def create_model(activation,neurons,dropout_rate):\n",
    "    inputlayer = layers.Input(shape=train.shape[1])\n",
    "    e2 = layers.Dense(500, activation=activation)(inputlayer)\n",
    "    e3 = layers.Dense(250, activation=activation)(e2)\n",
    "    dropout = layers.Dropout(dropout_rate)(e3)\n",
    "    e4 = layers.Dense(100)(dropout)\n",
    "\n",
    "    encoded = layers.Dense(neurons)(e4)\n",
    "\n",
    "    d1 = layers.Dense(100, activation=activation)(encoded)\n",
    "    d2 = layers.Dense(250, activation=activation)(d1)\n",
    "    d3 = layers.Dense(500, activation=activation)(d2)\n",
    "    outputlayer = layers.Dense(1101, activation=activation)(d3)\n",
    "\n",
    "    model = keras.models.Model(inputlayer, outputlayer, name=\"autoencoder_dense_v1\")\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which parameters do affect the model's performance significantly? Those will then after be optimized in grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Checking for the batch size impact (by choosing extreme values, while keeping the rest normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.00013501251160050743.\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.04583813098725e-05.\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 6.060711421014276e-05.\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 4.060676725202939e-05.\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.720653359574499e-05.\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.8228377484774683e-05.\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2213012305437588e-05.\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.182718220268725e-06.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.482421311171493e-06.\n",
      "Epoch 00029: early stopping\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 3.673222290672129e-06.\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 2.4610588616269524e-06.\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.648909385494335e-06.\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 1.1047692623833428e-06.\n",
      "Epoch 00010: early stopping\n",
      "\n",
      "Epoch 00003: ReduceLROnPlateau reducing learning rate to 7.401953905628034e-07.\n",
      "Epoch 00004: early stopping\n",
      "Val_loss with batch size 32: 0.4371951675415039\n",
      "Val_loss with batch size 128: 0.43719440937042237\n",
      "Val_loss with batch size 256: 0.43719438552856443\n"
     ]
    }
   ],
   "source": [
    "const_activation = 'tanh'\n",
    "const_drop = 0\n",
    "const_neurons = 50\n",
    "\n",
    "model_var_bs = create_model(const_activation, const_neurons,  const_drop)\n",
    "\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "rl_on_plateau= keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.67, patience=2,\n",
    "                                                 verbose=1, min_lr=1e-7)\n",
    "def fit_model(model, batch_size, verbose):\n",
    "    results = model.fit(train, train,\n",
    "                        epochs=50,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.1,\n",
    "                        verbose=verbose,\n",
    "                        callbacks=[earlystopping, rl_on_plateau])\n",
    "    return results\n",
    "\n",
    "results32 = fit_model(model_var_bs, 32, 0)\n",
    "results128 = fit_model(model_var_bs, 128, 0)\n",
    "results256 = fit_model(model_var_bs, 256, 0)\n",
    "\n",
    "#Comparing the validation loss\n",
    "print(\"Val_loss with batch size 32:\", results32.history['val_loss'][-1])\n",
    "print(\"Val_loss with batch size 128:\", results128.history['val_loss'][-1])\n",
    "print(\"Val_loss with batch size 256:\", results256.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.) Checking for the activation function impact (we try out tanh, relu with last activation=linear, and elu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00013501251160050743.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.04583813098725e-05.\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.060711421014276e-05.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.060676725202939e-05.\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.720653359574499e-05.\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.8228377484774683e-05.\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2213012305437588e-05.\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.182718220268725e-06.\n",
      "Epoch 00027: early stopping\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00013501251160050743.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.04583813098725e-05.\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.060711421014276e-05.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.060676725202939e-05.\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.720653359574499e-05.\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.8228377484774683e-05.\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2213012305437588e-05.\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.182718220268725e-06.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.482421311171493e-06.\n",
      "Epoch 00028: early stopping\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00013501251160050743.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.04583813098725e-05.\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.060711421014276e-05.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.060676725202939e-05.\n",
      "Epoch 00018: early stopping\n",
      "Val_loss with tanh: 0.43726702928543093\n",
      "Val_loss with elu: 0.00010671236959751696\n",
      "Val_loss with relu and linear: 0.00010903983755270019\n"
     ]
    }
   ],
   "source": [
    "const_bs = 128\n",
    "model_tanh = create_model(\"tanh\", const_neurons,  const_drop)\n",
    "model_elu = create_model(\"elu\", const_neurons, const_drop)\n",
    "\n",
    "#special model with mixed relu and linear\n",
    "inputlayer = layers.Input(shape=train.shape[1])\n",
    "e2 = layers.Dense(500, activation=\"relu\")(inputlayer)\n",
    "e3 = layers.Dense(250, activation=\"relu\")(e2)\n",
    "e4 = layers.Dense(100)(e3)\n",
    "encoded = layers.Dense(50)(e4)\n",
    "d1 = layers.Dense(100, activation=\"relu\")(encoded)\n",
    "d2 = layers.Dense(250, activation=\"relu\")(d1)\n",
    "d3 = layers.Dense(500, activation=\"relu\")(d2)\n",
    "outputlayer = layers.Dense(1101, activation=\"linear\")(d3)\n",
    "model_relu = keras.models.Model(inputlayer, outputlayer, name=\"autoencoder_dense_v1\")\n",
    "model_relu.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
    "\n",
    "\n",
    "results_tanh = fit_model(model_tanh, const_bs, 0)\n",
    "results_elu = fit_model(model_elu, const_bs, 0)\n",
    "results_relu = fit_model(model_relu, const_bs, 0)\n",
    "\n",
    "#Comparing the validation loss\n",
    "print(\"Val_loss with tanh:\", results_tanh.history['val_loss'][-1])\n",
    "print(\"Val_loss with elu:\", results_elu.history['val_loss'][-1])\n",
    "print(\"Val_loss with relu and linear:\", results_relu.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see hat elu performs best (similar to relu with linear). Therefore we take elu for the next parameters. Now we want to see if more or fewer neurons work better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00013501251160050743.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.04583813098725e-05.\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.060711421014276e-05.\n",
      "Epoch 00016: early stopping\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "Epoch 00011: early stopping\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "Epoch 00009: early stopping\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "Epoch 00005: early stopping\n",
      "Val_loss with 2 neurons: 0.00010930380667559803\n",
      "Val_loss with 10 neurons: 0.00010828426806256175\n",
      "Val_loss with 25 neurons: 0.00010824832395883277\n",
      "Val_loss with 50 neurons: 0.00010777310322737321\n",
      "Val_loss with 100 neurons: 0.0001095503306714818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Performance of model for different latent space neurons')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEWCAYAAADoyannAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3de5xdVX3//9c7V4ggqUkUSCATJVYSpYgjRGkFQX4kKAQrlqGxIIVvvloiX1uVgui3v1LzrfjzWywVsEGQS0cCjYojDZciIIjcJgKRECMjuUqEcAuEKDjw+f2x1sjO8Zw5Z4Y5M9mT9/PxOI85e+211/7syzmfs/Zec44iAjMzszIaMdQBmJmZ9ZeTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTWIlI+pKkJyX9eqhj2R5IOljSI5K2SDp2kNd9maQvNVh3jaQP1Ji3s6QfSNos6T8HNsrfryMk7ZOff0PSFwvzPinp8bwPJwzlPm2EpNsknTrUcdj2w0msifKb12/yG8Ljkr4laZd+trUX8BlgRkTsPrCRltY5wNcjYpeIuHaog+mn44A3ARMi4qPNXllEfCIi/glA0mjgX4D/J+/DpxjCfSrp45J+PIDt1fzw0I+2WvKHgVED0Z4NHCex5js6InYBDgDeDXyhrw3kF85U4KmIeKKfyw9HU4EVQx3EazQV+EVEdPd1wQE4rm8CdmLbfdjvfTqMzzPrA0kjB3WFEeFHkx7AGuADhen/D7guP98NuATYCPwK+BIwMs/7OHAncB7wNPBj4DfAK8AW4LJc7xjSG86zwG3AvhXr/ntgOfAiMCqXfS6XvZDX/ybgeuB54Gbgjwpt/Cfwa2AzcDswszDvMuAC4L/ysvcAbynMnwn8d47/ceDzuXwEcCbwS+Ap4BrgDb3sw/8BdOV2OoA9c/kv8/74Td4nY2vs/75sb2/7853AT/NyVwOLgS8V5n8IeCAv+xNgv1rnQaH8H4GXgN/lbTgl758vAGuBJ4ArgN1y/RYgcr11wO019tnnSOfVY8Bf52X2KRy3LwFvzfsk8rpvqbZP6dt5+qVc/tfASuAZ4EZgaiG2AD4BPJLnXwAI2Bf4LfByXvezNbbtNuDU/PwtOe6ngCeBdmB8nndlxbackctn5ePzLPAgcGhF2/+Ut+l54CZgYp63rrCvtgDvqRLbgUAn8BzpnP+XiuM2Px+TjcBnKpa7K8e0Efg6MGYgX0vAocAG0tWcJ/J6Ti7MHwt8NW/n48A3gJ0Lx/nHFe1VnlMXAUtJ59QHSOfNFcAm0rn8BWBEsb28vmeA1cCcQtsfBx7Nx2A1MK/X99mhfqMfzg8Kb17AXqQ3yH/K09cC/w68DngjcC/wPwsHsRv4FCn57NxzEhba7nkTOgIYDZxBerMfU1j3A3m9OxfK7ia9kU/OJ/NPSW/QY0lvCP9QWMdfA7vmeV8DHijMuyy/qA7MMbYDi/O8XfOL5DOkT/q7AgfleZ/OMUzJ7f47cFWN/XcY6c3pgFz33yi8cVMjOVTMb2h7e9uf+bEW+Ns87zhS4ul50z4gt30QMBI4Ka97bL04gf8X+I+Kfd4FvBnYBfgucGWe10J687iCdN7sXKW92aQ3obfnOt+mShKraG9UrX1K38/TY3P8++ayLwA/KbQXwHXAeGBv0pvc7EJ7P651PHOd23g1ie2Tj9dYYBLpg9bXetmWyaQ3+6NICeCIPD2p0PYv87mwc57+cq19VSW2u4C/ys93AWZVLHtV3o/vyNvd897wLlJyHZXrrgQ+PcCvpUPzsTqHdA4fBWwlf4gjvb47gDfkdfwA+Odax4U/PKc2Awfn/boT6Rz9fm6rBfgFcEqhvd+RPqCOBD5JSu7K++c54I9z3T0ofHiuum1D9Qa/Izzyi2gL6RPWWuDC/OJ4E6l3tHOh7gnArYWDvK7KSVhMYl8ErilMjyB9Uj60sO6/rhLPvML0d4CLCtOfAq6tsS3j84nb0yu4DPhmYf5RwM8L23J/jXZWAocXpvfIJ/QfvDmQegBfKUzvkuu2FLanXhJraHt725/A+3peZIX5P+HVZHAR+cNJYf4q4JB6cfKHSeyHwN8Upv+4Z//w6pvhm3vZ5kvJb7x5+q30M4nRv/P0evKbVWE/biX3xvL6/rQw/xrgzEJ7DSexKvOOLZ53lfuddGXiyoplbgROKrT9hcK8vwFuqLWvqqz/dlLvemJFec+ybyuUfQW4pEY7nwa+N8CvpUNJvdLisX6ClDxF+gBXvJLyHmB1reNS5Zy6ojBvZD5vZhTK/idwW6G9rsK8cbm93UlJ7FngI1T5kFbt4WvYzXdsRNxcLJD0DtKnoY2SeopHAOsL1YrPq9mTlBgBiIhXJK0nfdrsrY3HC89/U2V6lxzjSGAh8FHSp9xXcp2JpE9dkC419tjasyyp9/fLGnFPBb4n6ZVC2cukN8xfVdTdk9RzAiAitkh6irSNa2q0X6mh7aX3/fky8KvIr7hsbeH5VOAkSZ8qlI3JbfbVNnHk56NI+6dHb+fGnsCyGnH21VT6fp5OBf5V0v8tlIm0H3tiqXXe9ImkNwLnA39G+sQ/gnR5qpapwEclHV0oGw3cWph+LbGdQurp/FzSauAfI+K6wvzivlpL6pEh6a2kATatpDf0Ubx6DAfqtQTpnnrx3mvP9k3K611WOM4iJaNGFbdtIq9eveixlm3fm36/nyNia17vLhHxa0nHA58FLpF0J+nS689rrdgDO4bGetInlYkRMT4/Xh8RMwt1osayPR4jncQAKJ0Fe7HtyVuvjd78JTCXV69vt/SsqoFl15PuV9SaN6ew3eMjYqeIqPaiq9zG1wETqP4Cfa16258bgckqvMJJl8J6rAcWVmzTuIi46rXGkdfTzbbJt7fjujHHXS3OvurPebqedLmxuC92joifNLC+vp6v/5yX2S8iXg98jG3Pz2qxXVkR2+si4ssDEVtEPBIRJ5Auu54LLMnnbI/K4/JYfn4R8HNget6Ozxe2Y6BeS715kvSBbmahnd0iDUiD1Esb11NZUrXR0cX98ySpR1h5HjcUV0TcGBFHkHqWPwcu7q2+k9gQiIiNpJvG/1fS6yWNkPQWSYf0oZlrgA9KOjwPlf4M6Q2nkTeLRuya23uKdAL/nz4sex2wu6RPSxoraVdJB+V53wAWSpoKIGmSpLk12vk2cLKk/SWNzTHcExFr+rE99fS2P+8iJZLTJY2S9Oeke4E9LgY+IekgJa+T9EFJu/YjjquAv5U0Lf87xv8Bro7GRy9eA3xc0gxJ44B/6EcMQL/P028AZ0maCSBpN0mN/uvA48AUSWMarL8r+XK9pMmkAS2V7b25MP0fwNGSjpQ0UtJOkg6VNKWBdW0iXY14c60Kkj4maVJEvEK6JAapZ9Tji5LG5X1zMmmAUM92PAdskfQ20j2iHgP1Wqopx3sxcF7u3SJpsqQjc5UHgZn5dbgT6RJ4b+29TDoPF+Z4pwJ/R9r/vZL0JknH5OT/Iun4vtzbMk5iQ+dEUpf7YdIlkCWkTx4NiYhVpE+e/0b65HM0aTj/SwMU3xWkSwC/yjHe3YfYnifdND+adNngEeD9efa/km4g3yTp+dzuQTXa+SHpXtV3SD2MtwBt/diWRmKuuT/zPv1z0rX8Z4DjSQMuepbtJN2k/nqe35Xr9selpJF1t5NGZv2WdO+u0e24nnST/pYcxy39jKNHn87TiPgeqReyWNJzwEPAnAbXdQtp8NOvJT3ZQP1/JA2q2UwaJfvdivn/DHxB0rOSPhsR60lXFz5PSkrrSYmv7vtgRGwlXV6/M7c3q0q12cAKSVtI53lbRPy2MP9HpGPyQ+CrEXFTLv8s6crH86Rk0pPcBuy11IC/z7HdnY/bzaT7sUTEL0iXSW/O62/kf/k+RerBPZrrf5t0btczgvQB8jHSwLFDSPcma9K2l/nNzGwgSWohfSAZ3YcetTXIPTEzMystJzEzMystX040M7PSck/MzMxKy//sPAgmTpwYLS0tQx2GmVmpLFu27MmImNRbHSexQdDS0kJnZ+dQh2FmViqS6n7jjC8nmplZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZnZwGtvh5YWGDEi/W1vb8pqPMTezMwGVns7zJ8PW7em6bVr0zTAvHkDuir3xMzMbGCdffarCazH1q2pfIA5iZmZ2cBat65v5a+Bk5iZmQ2svffuW/lr4CRmZmYDa+FCGDdu27Jx41L5AGt6EpM0W9IqSV2Szqwyf6ykq/P8e/KvoPbMOyuXr5J0ZL02JS3IZSFpYqFcks7P85ZLOqAw71xJD+XH8YXyyyStlvRAfuxfry0zMyMN3li0CKZOBSn9XbRowAd1QJNHJ0oaCVwAHAFsAO6T1BERDxeqnQI8ExH7SGoDzgWOlzQDaANmAnsCN0t6a16mVpt3AtcBt1WEMgeYnh8HARcBB0n6IHAAsD8wFviRpOsj4rm83OciYkkjbfVrB5mZDVfz5jUlaVVqdk/sQKArIh6NiJeAxcDcijpzgcvz8yXA4ZKUyxdHxIsRsRroyu3VbDMi7o+INVXimAtcEcndwHhJewAzgB9FRHdEvAA8CMyus0212jIzs0HW7CQ2GVhfmN6Qy6rWiYhuYDMwoZdlG2mz0TgeBOZIGpcvP74f2KtQb2G+ZHiepLF92CYkzZfUKalz06ZNdcIzM7P+aHYSU5WyaLBOX8v7HEdE3AQsBX4CXAXcBXTn+WcBbwPeDbwB+Ps68VY2vigiWiOiddKkXn/TzczM+qnZSWwD2/ZspgCP1aojaRSwG/B0L8s20mbDcUTEwojYPyKOICWoR3L5xnzJ8EXgW6TLmI1uk5mZDYJmJ7H7gOmSpkkaQxqo0VFRpwM4KT8/DrglIiKXt+XRi9NIAynubbDNSh3AiXlk4Sxgc0RslDRS0gQASfsB+wE35ek98l8BxwIP9dZWP/aNmZm9Rk0dnRgR3ZIWADcCI4FLI2KFpHOAzojoAC4BrpTUReqBteVlV0i6BniYdInvtIh4GdJQ+so2c/npwBnA7sBySUsj4lTSJcOjSINDtgIn5xBHA3ekPMVzwMfyfTmAdkmTSL2zB4BP5PJabZmZ2SBT6vRYM7W2tkZnZ+dQh2FmViqSlkVEa291/I0dZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5iZmZWWk5i26v2dmhpgREj0t/29qGOyMxsu9PU3xOzfmpvh/nzYevWNL12bZoGmDdv6OIyM9vOuCe2PTr77FcTWI+tW1O5mZn9npPY9mjdur6Vm5ntoJzEtkd77923cjOzHVTTk5ik2ZJWSeqSdGaV+WMlXZ3n3yOppTDvrFy+StKR9dqUtCCXhaSJhXJJOj/PWy7pgMK8cyU9lB/HF8rb8zoeknSppNG5/FBJmyU9kB//eyD3FwALF8K4cduWjRuXys3M7PeamsQkjQQuAOYAM4ATJM2oqHYK8ExE7AOcB5ybl50BtAEzgdnAhZJG1mnzTuADwNqKdcwBpufHfOCivI4PAgcA+wMHAZ+T9Pq8TDvwNuAdwM7AqYX27oiI/fPjnP7sm17NmweLFsHUqSClv4sWeVCHmVmFZvfEDgS6IuLRiHgJWAzMragzF7g8P18CHC5JuXxxRLwYEauBrtxezTYj4v6IWFMljrnAFZHcDYyXtAcpCf4oIroj4gXgQVLCJCKW5voB3AtMGZA90qh582DNGnjllfTXCczM7A80O4lNBtYXpjfksqp1IqIb2AxM6GXZRtpsNI4HgTmSxuXLj+8H9ioumC8j/hVwQ6H4PZIelHS9pJl11m1mZk3S7P8TU5WyaLBOrfJqibeyzYbiiIibJL0b+AmwCbgL6K6odyFwe0Tckad/CkyNiC2SjgKuJV2m3HaF0nzSpUv29oAMM7OmaHZPbAPb9mymAI/VqiNpFLAb8HQvyzbSZsNxRMTCfG/rCFKye6SnkqR/ACYBf9dTFhHPRcSW/HwpMLo4iKRQb1FEtEZE66RJk+qEZ2Zm/dHsJHYfMF3SNEljSAM1OirqdAAn5efHAbfk+1AdQFsevTiN1Nu5t8E2K3UAJ+ZRirOAzRGxMQ8UmQAgaT9gP+CmPH0qcCRwQkS80tOQpN3zPTskHUjah0/1fdeYmdlr1dTLiRHRLWkBcCMwErg0IlZIOgfojIgO4BLgSkldpB5YW152haRrgIdJl/hOi4iXIQ2lr2wzl58OnAHsDiyXtDQiTgWWAkeRBodsBU7OIY4G7sg56TngY/m+HMA3SKMc78rzv5tHIh4HfFJSN/AboC0nXTMzG2Ty+2/ztba2Rmdn51CHYWZWKpKWRURrb3X8jR1mZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTU9ikmZLWiWpS9KZVeaPlXR1nn+PpJbCvLNy+SpJR9ZrU9KCXBaSJhbKJen8PG+5pAMK886V9FB+HF8on5bjeSTHN6ZevGZmNriamsQkjQQuAOYAM4ATJM2oqHYK8ExE7AOcB5ybl50BtAEzgdnAhZJG1mnzTuADwNqKdcwBpufHfOCivI4PAgcA+wMHAZ+T9Pq8zLnAeRExHXgmx1kzXjMzG3zN7okdCHRFxKMR8RKwGJhbUWcucHl+vgQ4XJJy+eKIeDEiVgNdub2abUbE/RGxpkocc4ErIrkbGC9pD1IS/FFEdEfEC8CDwOy8/sNyPOT4jq0Tr5mZDbJmJ7HJwPrC9IZcVrVORHQDm4EJvSzbSJuNxvEgMEfSuHz58f3AXnn9z+Z4KtdRK95tSJovqVNS56ZNm+qEZ2Zm/dHsJFathxIN1ulreZ/jiIibgKXAT4CrgLuA7jrraGj9EbEoIlojonXSpEl1wjMzs/5odhLbQOrZ9JgCPFarjqRRwG7A070s20ibDccREQsjYv+IOIKUoB4BniRdchxVZR214jUzs0HW7CR2HzA9j/QbQxqo0VFRpwM4KT8/DrglIiKXt+XRgNNIgzLubbDNSh3AiXmU4ixgc0RszANFJgBI2g/YD7gpr//WHA85vu/XidfMzAbZqPpV+i8iuiUtAG4ERgKXRsQKSecAnRHRAVwCXCmpi9SjacvLrpB0DfAw6RLfaRHxMqSh9JVt5vLTgTOA3YHlkpZGxKmkS4ZHkQaHbAVOziGOBu7I4zKeAz5WuA/298BiSV8C7s9xUiteMzMbfHInovlaW1ujs7NzqMMwMysVScsiorW3Ov7GDjMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzK62Gkpikj0raNT//gqTvSjqguaGZmZn1rtGe2Bcj4nlJfwocCVwOXNS8sMzMzOprNIm9nP9+ELgoIr4PjGlOSGZmZo1pNIn9StK/A38BLJU0tg/LmpmZNUWjiegvgBuB2RHxLPAG4HONLChptqRVkroknVll/lhJV+f590hqKcw7K5evknRkvTYlLchlIWlioVySzs/zlhfv50n6iqQVklbmOpK0q6QHCo8nJX0t1/+4pE2Feac2uA/NzGyAjWqw3h7Af0XEi5IOBfYDrqi3kKSRwAXAEcAG4D5JHRHxcKHaKcAzEbGPpDbgXOB4STOANmAmsCdws6S35mVqtXkncB1wW0Uoc4Dp+XEQ6X7eQZLeCxyctwfgx8AhEXEbsH9hO5YB3y20d3VELKi3/WZm1lyN9sS+A7wsaR/gEmAa8O0GljsQ6IqIRyPiJWAxMLeizlzSQBGAJcDhkpTLF0fEixGxGujK7dVsMyLuj4g1VeKYC1wRyd3AeEl7AAHsRLq/NxYYDTxeXFDSdOCNwB0NbK+ZmQ2iRpPYKxHRDfw58LWI+FtS76yeycD6wvSGXFa1Tl7HZmBCL8s20mZDcUTEXcCtwMb8uDEiVlYsewKp5xWFso/ky5JLJO1VbYWS5kvqlNS5adOmOuGZmVl/NJrEfifpBOBE0uU6SL2WelSlLBqs09fyPseRe5b7AlNIie4wSe+rqNcGXFWY/gHQEhH7ATfzai9y28YjFkVEa0S0Tpo0qU54ZmbWH40msZOB9wALI2K1pGnAfzSw3Aag2FOZAjxWq46kUcBuwNO9LNtIm43G8WHg7ojYEhFbgOuBWT2VJP0JMCoilvWURcRTEfFinrwYeFeddZuZWZM0lMTyoInPAj+T9HZgQ0R8uYFF7wOmS5omaQypV9NRUacDOCk/Pw64JV+66wDa8ujFaaRBGfc22GalDuDEPPJwFrA5IjYC64BDJI2SNBo4BCheTjyBbXth5HtpPY6pqG9mZoOoodGJeUTi5cAa0qW5vSSdFBG397ZcRHRLWkAanj8SuDQiVkg6B+iMiA7SQJErJXWRemBtedkVkq4BHga6gdMi4uUczx+0mctPB84AdgeWS1oaEacCS4GjSINDtpJ6lpAGkhwG/Ix0SfKGiPhBYRP+Ii9XdLqkY3JMTwMfr78HzcysGbTteIUaldIQ87+MiFV5+q3AVRHhS2kNaG1tjc7OzqEOw8ysVCQti4jW3uo0ek9sdE8CA4iIX9DYwA4zM7OmafSfnTslXQJcmafnAct6qW9mZtZ0jSaxTwKnAaeT7ondDlzYrKDMzMwa0VASy0PK/yU/zMzMtgu9JjFJPaP2qsr/8GtmZjYk6vXEPjQoUZiZmfVDr0ksItY20oikuyLiPQMTkpmZWWMG6octdxqgdszMzBo2UEms/n9Mm5mZDbCBSmJmZmaDbqCSWLWfOjEzM2uqgUpifzVA7ZiZmTWs3v+JPU/1+10CIiJeT3ryUBNiMzMz61W9Ifa7DlYgZmZmfdXodycCIOmNFIbTR8S6AY/IzMysQQ3dE5N0jKRHgNXAj0g/jnl9E+MyMzOrq9GBHf8EzAJ+ERHTgMOBO5sWlZmZWQMaTWK/i4ingBGSRkTErcD+TYzLzMysrkbviT0raRfgDqBd0hNAd/PCMjMzq6/Xnpikr0s6GJgLbAU+DdwA/BI4upEVSJotaZWkLklnVpk/VtLVef49kloK887K5askHVmvTUkLcllImlgol6Tz87zlkg4ozPuKpBWSVuY6yuW35XU8kB9vrBevmZkNrno9sUeArwJ7AFcDV0XE5Y02LmkkcAFwBLABuE9SR0Q8XKh2CvBMROwjqQ04Fzhe0gygDZgJ7AncLOmteZlabd4JXAfcVhHKHGB6fhwEXAQcJOm9wMFAz++i/Rg4pLD8vIjorGiraryN7hMzMxs4vfbEIuJf80+sHAI8DXwr91i+WEgovTkQ6IqIRyPiJWAxqVdXNBfoSYxLgMNzb2gusDgiXoyI1UBXbq9mmxFxf0SsqRLHXOCKSO4Gxkvag/SP3DsBY4CxwGjg8TrbVCteMzMbZA0N7IiItRFxbkS8E/hL4M+BlQ0sOhlYX5jekMuq1omIbmAzMKGXZRtps6E4IuIu4FZgY37cGBHF7fpWvpT4xUKiqhXvNiTNl9QpqXPTpk11wjMzs/5o9P/ERks6WlI76f/DfgF8pJFFq5RVfo1VrTp9Le9zHJL2AfYFppCS02GS3pfnz4uIdwB/lh893w/Z0PojYlFEtEZE66RJk+qEZ2Zm/VFvYMcRki4l9VzmA0uBt0TE8RFxbQPtbwD2KkxPAR6rVUfSKGA30qXLWss20majcXwYuDsitkTEFlKCngUQEb/Kf58Hvk26jNlbvGZmNsjq9cQ+D9wF7BsRR0dEe0S80If27wOmS5omaQxpoEZHRZ0O4KT8/DjgloiIXN6WRwNOIw3KuLfBNit1ACfmUYqzgM0RsRFYBxwiaZSk0aR7fyvz9ERIvVDgQ8BDhbaqxWtmZoOs3hcAv/+1NB4R3ZIWADcCI4FLI2KFpHOAzojoAC4BrpTURerRtOVlV0i6BniY9D9pp0XEy5CG0le2mctPB84AdgeWS1oaEaeSepBHkQaHbAVOziEuAQ4Dfka6JHhDRPxA0uuAG3MCGwncDFycl6kar5mZDT65E9F8ra2t0dlZOVLfzMx6I2lZRLT2VmegfhTTzMxs0DmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJ7cja26GlBUaMSH/b24c6IjOzPun1l51tGGtvh/nzYevWNL12bZoGmDdv6OIyM+sD98R2VGef/WoC67F1ayo3MysJJ7Ed1bp1fSs3M9sONT2JSZotaZWkLklnVpk/VtLVef49kloK887K5askHVmvTUkLcllImlgol6Tz87zlkg4ozPuKpBWSVuY6kjRO0n9J+nme9+VC/Y9L2iTpgfw4dWD32CDZe+++lW9vfD/PzGhyEpM0ErgAmAPMAE6QNKOi2inAMxGxD3AecG5edgbQBswEZgMXShpZp807gQ8AayvWMQeYnh/zgYvyOt4LHAzsB7wdeDdwSF7mqxHxNuCdwMGS5hTauzoi9s+Pb/Zr5wy1hQth3Lhty8aNS+Xbu577eWvXQsSr9/OcyMx2OM3uiR0IdEXEoxHxErAYmFtRZy5weX6+BDhcknL54oh4MSJWA125vZptRsT9EbGmShxzgSsiuRsYL2kPIICdgDHAWGA08HhEbI2IW3ObLwE/BaYMwP7YfsybB4sWwdSpIKW/ixaVY1CH7+eZWdbsJDYZWF+Y3pDLqtaJiG5gMzChl2UbabOhOCLiLuBWYGN+3BgRK4sLShoPHA38sFD8kXxZcomkvaqtUNJ8SZ2SOjdt2lQnvCEybx6sWQOvvJL+liGBge/nmdnvNTuJqUpZNFinr+V9jkPSPsC+pF7WZOAwSe/7/ULSKOAq4PyIeDQX/wBoiYj9gJt5tRe5beMRiyKiNSJaJ02aVCc868RUii4AAA2NSURBVJOy388zswHT7CS2ASj2VKYAj9Wqk5PGbsDTvSzbSJuNxvFh4O6I2BIRW4DrgVmFeouARyLiaz0FEfFURLyYJy8G3lVn3TbQynw/z8wGVLOT2H3AdEnTJI0hDdToqKjTAZyUnx8H3BIRkcvb8ujFaaRBGfc22GalDuDEPPJwFrA5IjYC64BDJI2SNJo0qGMlgKQvkRLqp4sN5XtpPY7pqW+DqMz388xsQDX1GzsiolvSAuBGYCRwaUSskHQO0BkRHcAlwJWSukg9sLa87ApJ1wAPA93AaRHxMqSh9JVt5vLTgTOA3YHlkpZGxKnAUuAo0uCQrcDJOcQlwGHAz0iXJG+IiB9ImgKcDfwc+GkaZ8LX80jE0yUdk2N6Gvh4E3ad1TNvnpOWmaHU6bFmam1tjc7OzqEOw8ysVCQti4jW3ur4GzvMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0nMTMzKy0mp7EJM2WtEpSl6Qzq8wfK+nqPP8eSS2FeWfl8lWSjqzXpqQFuSwkTSyUS9L5ed5ySQcU5n1F0gpJK3Md5fJ3SfpZXqZY/gZJ/y3pkfz3jwZ6n5mZWWOamsQkjQQuAOYAM4ATJM2oqHYK8ExE7AOcB5ybl50BtAEzgdnAhZJG1mnzTuADwNqKdcwBpufHfOCivI73AgcD+wFvB94NHJKXuSjX7Vludi4/E/hhREwHfpinzcxsCDS7J3Yg0BURj0bES8BiYG5FnbnA5fn5EuDw3OuZCyyOiBcjYjXQldur2WZE3B8Ra6rEMRe4IpK7gfGS9gAC2AkYA4wFRgOP53mvj4i7IiKAK4Bjq8R7eaHczMwGWbOT2GRgfWF6Qy6rWiciuoHNwIRelm2kzYbiiIi7gFuBjflxY0SszPU31FjHmyJiY453I/DGaiuUNF9Sp6TOTZs21QnPzMz6o9lJTFXKosE6fS3vcxyS9gH2BaaQktRhkt7Xz3VsWzliUUS0RkTrpEmT+rKomZk1qNlJbAOwV2F6CvBYrTqSRgG7AU/3smwjbTYax4eBuyNiS0RsAa4HZuX6U2qso+dyI/nvE3XWbWZmTdLsJHYfMF3SNEljSAM1OirqdAAn5efHAbfk+1AdQFsevTiNNLji3gbbrNQBnJhHKc4CNudLgeuAQySNkjSaNKhjZZ73vKRZ+f7cicD3q8R7UqHczMwGWVOTWL7HtQC4EVgJXBMRKySdI+mYXO0SYIKkLuDvyKP9ImIFcA3wMHADcFpEvFyrTQBJp0vq6UUtl/TNvI6lwKOkwSEXA3+Ty5cAvwR+BjwIPBgRP8jzPgl8My/zS1IvDeDLwBGSHgGOyNNmZjYElDo91kytra3R2dk51GGYmZWKpGUR0dpbHX9jh5mZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmJmZlZaTmNmOor0dWlpgxIj0t719qCMye81GDXUAZjYI2tth/nzYujVNr12bpgHmzRu6uMxeo6b3xCTNlrRKUpekM6vMHyvp6jz/HkkthXln5fJVko6s16akBbksJE0slEvS+XneckkH5PL3S3qg8PitpGPzvDsK5Y9JujaXHyppc2He/27GfjMbUGef/WoC67F1ayo3K7Gm9sQkjQQuAI4ANgD3SeqIiIcL1U4BnomIfSS1AecCx0uaAbQBM4E9gZslvTUvU6vNO4HrgNsqQpkDTM+Pg4CLgIMi4lZg/xzrG4Au4CaAiPizwnZ8B/h+ob07IuJD/d4xZoNt3bq+lZuVRLN7YgcCXRHxaES8BCwG5lbUmQtcnp8vAQ6XpFy+OCJejIjVpARzYG9tRsT9EbGmShxzgSsiuRsYL2mPijrHAddHxDYfVyXtChwGXNuP7TfbPuy9d9/KzUqi2UlsMrC+ML0hl1WtExHdwGZgQi/LNtJmf+JoA66qsuyHgR9GxHOFsvdIelDS9ZJmVluhpPmSOiV1btq0qU54Zk22cCGMG7dt2bhxqdysxJqdxFSlLBqs09fyfseRe2XvAG6sUu8Etk1uPwWmRsSfAP9GjR5aRCyKiNaIaJ00aVKd8MyabN48WLQIpk4FKf1dtMiDOqz0mj06cQOwV2F6CvBYjTobJI0CdgOerrNsvTb7GsdfAN+LiN8VF5I0gXT58sM9ZcUeWUQslXShpIkR8WSdGMyG1rx5Tlo27DS7J3YfMF3SNEljSJfsOirqdAAn5efHAbdEROTytjx6cRppUMa9DbZZqQM4MY9SnAVsjoiNhfmVva0eHwWui4jf9hRI2j3fs0PSgaR9+FSd9ZuZWRM0tScWEd2SFpAu040ELo2IFZLOATojogO4BLhSUhepB9aWl10h6RrgYaAbOC0iXoY0lL6yzVx+OnAGsDuwXNLSiDgVWAocRRocshU4uSfGPKR/L+BHVTahDfhyRdlxwCcldQO/Adpy0jUzs0Emv/82X2tra3R2dg51GGZmpSJpWUS09lbHXztlZmal5SRmZmal5cuJg0DSJmBtnWoTgR1xhKO3e8eyo2437Ljb/lq2e2pE9Po/Sk5i2wlJnfWu/Q5H3u4dy4663bDjbnuzt9uXE83MrLScxMzMrLScxLYfi4Y6gCHi7d6x7KjbDTvutjd1u31PzMzMSss9MTMzKy0nMTMzKy0nse2ApNmSVknqknTmUMfTLJL2knSrpJWSVkj6X7n8DZL+W9Ij+e8fDXWszSBppKT7JV2Xp6dJuidv99X5C62HFUnjJS2R9PN83N+zIxxvSX+bz/GHJF0laafheLwlXSrpCUkPFcqqHt/8Bezn5/e55ZIOGIgYnMSGmKSRwAXAHGAGcIKkGUMbVdN0A5+JiH2BWcBpeVvPJP3w6HTgh3l6OPpfwMrC9LnAeXm7nwFOGZKomutfgRsi4m3An5C2f1gfb0mTgdOB1oh4O+mLytsYnsf7MmB2RVmt4zuH9Gsk04H5wEUDEYCT2NA7EOiKiEcj4iVgMTB3iGNqiojYGBE/zc+fJ72hTSZt7+W52uXAsUMTYfNImgJ8EPhmnhZwGLAkVxl22y3p9cD7SL9UQUS8FBHPsgMcb9IvhOycfyNxHLCRYXi8I+J20q+PFNU6vnOBKyK5Gxiff5D4NXESG3qTgfWF6Q25bFjLP4HzTuAe4E09v++W/75x6CJrmq+RfibolTw9AXg2Irrz9HA87m8GNgHfypdRvynpdQzz4x0RvwK+CqwjJa/NwDKG//HuUev4NuW9zkls6KlK2bD+vwdJuwDfAT5d/KXs4UrSh4AnImJZsbhK1eF23EcBBwAXRcQ7gRcYZpcOq8n3gOYC04A9gdeRLqVVGm7Hu56mnPNOYkNvA+lHOXtMAR4boliaTtJoUgJrj4jv5uLHey4r5L9PDFV8TXIwcIykNaTLxYeRembj8+UmGJ7HfQOwISLuydNLSEltuB/vDwCrI2JTRPwO+C7wXob/8e5R6/g25b3OSWzo3QdMzyOXxpBuAHcMcUxNke8DXQKsjIh/KczqAE7Kz08Cvj/YsTVTRJwVEVMiooV0fG+JiHnAraRfCofhud2/BtZL+uNcdDjpl9qH9fEmXUacJWlcPud7tntYH++CWse3Azgxj1KcBWzuuez4WvgbO7YDko4ifTIfCVwaEQuHOKSmkPSnwB3Az3j13tDnSffFrgH2Jr0BfDQiKm8WDwuSDgU+GxEfkvRmUs/sDcD9wMci4sWhjG+gSdqfNJhlDPAocDLpw/OwPt6S/hE4njQi937gVNL9n2F1vCVdBRxK+rmVx4F/AK6lyvHNCf3rpNGMW4GTI+I1/+S9k5iZmZWWLyeamVlpOYmZmVlpOYmZmVlpOYmZmVlpOYmZmVlpOYmZmVlpOYmZWc+vKZiVjpOYWclIasm/zXVx/s2qmyTtLOktkm6QtEzSHZLelutfJum4wvJb8t9D8++7fZv0D+hI+rv8G1gPSfp0b+vL806X9HD+fajFg74zbIfnJGZWTtOBCyJiJvAs8BFgEfCpiHgX8FngwgbaORA4OyJmSHoX6Rs1DiL93tv/kPTOXtYH6Qt93xkR+wGfGJhNM2vcqPpVzGw7tDoiHsjPlwEtpC+Z/c/07T4AjG2gnXsjYnV+/qfA9yLiBQBJ3wX+jPSdd9XWB7AcaJd0LenrhswGlZOYWTkVv3PvZeBNpN+r2r9K3W7yVZf8/XVjCvNeKDyv9lMZtda3c37+QdIPXx4DfFHSzMJvZpk1nS8nmg0PzwGrJX0UUrKS9Cd53hrgXfn5XGB0jTZuB47N377+OuDDpC9srkrSCGCviLiV9IOf44FdXuuGmPWFk5jZ8DEPOEXSg8AKUsICuBg4RNK9pPtdL1RbOCJ+ClwG3Ev6ZYFvRsT9vaxvJPAfkn5G+lb28yLi2YHYELNG+VvszcystNwTMzOz0nISMzOz0nISMzOz0nISMzOz0nISMzOz0nISMzOz0nISMzOz0vr/AUvHzrdKaxV1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_neurons_2 = create_model(\"elu\", 2, 0)\n",
    "model_neurons_10 = create_model(\"elu\", 10, 0)\n",
    "model_neurons_25 = create_model(\"elu\", 25, 0)\n",
    "model_neurons_50 = create_model(\"elu\", 50, 0)\n",
    "model_neurons_100 = create_model(\"elu\", 100, 0)\n",
    "\n",
    "models_neurons = {model_neurons_2: 2, model_neurons_10: 10, model_neurons_25: 25, \n",
    "                  model_neurons_50: 50, model_neurons_100: 100}\n",
    "results = {}\n",
    "\n",
    "#training the models\n",
    "for model_name, n in models_neurons.items():\n",
    "    result = fit_model(model_name, 128, 0)\n",
    "    results[model_name] = result\n",
    "\n",
    "#printing the validation losses for the models\n",
    "for model_name, n in models_neurons.items():\n",
    "    print(f\"Val_loss with {n} neurons:\", results[model_name].history['val_loss'][-1])\n",
    "\n",
    "\n",
    "\n",
    "#plotting the val_loss for different neurons \n",
    "plt.plot(list(models_neurons.values()), [results[model_name].history['val_loss'][-1] for model_name in results.keys()], 'ro')\n",
    "plt.ylabel(\"Val_loss\")\n",
    "plt.xlabel(\"neurons\")\n",
    "plt.title(\"Performance of model for different latent space neurons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the model with 50 neurons performs best. Lastly, we want to see if a dropout layer in the model can improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "\n",
      "Epoch 00012: ReduceLROnPlateau reducing learning rate to 0.00013501251160050743.\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 9.04583813098725e-05.\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 6.060711421014276e-05.\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.060676725202939e-05.\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 2.720653359574499e-05.\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.8228377484774683e-05.\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.2213012305437588e-05.\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 8.182718220268725e-06.\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 5.482421311171493e-06.\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 3.673222290672129e-06.\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 2.4610588616269524e-06.\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.648909385494335e-06.\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.1047692623833428e-06.\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 7.401953905628034e-07.\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 4.959309109153764e-07.\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.3227371602606583e-07.\n",
      "Epoch 00042: early stopping\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Epoch 00004: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "Epoch 00007: early stopping\n",
      "\n",
      "Epoch 00005: ReduceLROnPlateau reducing learning rate to 0.0006700000318232924.\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.0004489000252215192.\n",
      "\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.0003007630087086.\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0002015112101798877.\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00013501251160050743.\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.04583813098725e-05.\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 6.060711421014276e-05.\n",
      "Epoch 00017: early stopping\n",
      "Val_loss with 0% dropout rate: 0.00010549005877692252\n",
      "Val_loss with 10.0% dropout rate: 0.00012050992052536457\n",
      "Val_loss with 30.0% dropout rate: 0.00012944667367264628\n",
      "Val_loss with 80.0% dropout rate: 0.00011758754961192608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Performance of model for different dropout rates')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dropout_0 = create_model(\"elu\", 50, 0)\n",
    "model_dropout_10 = create_model(\"elu\", 50, 0.1)\n",
    "model_dropout_30 = create_model(\"elu\", 50, 0.3)\n",
    "model_dropout_80 = create_model(\"elu\", 50, 0.8)\n",
    "\n",
    "models_dropouts = {model_dropout_0: 0, \n",
    "                   model_dropout_10: 0.1, \n",
    "                   model_dropout_30: 0.3, \n",
    "                   model_dropout_80: 0.8}\n",
    "results = {}\n",
    "\n",
    "#training the models\n",
    "for model_name, n in models_dropouts.items():\n",
    "    result = fit_model(model_name, 128, 0)\n",
    "    results[model_name] = result\n",
    "\n",
    "#printing the validation losses for the models\n",
    "for model_name, n in models_dropouts.items():\n",
    "    print(f\"Val_loss with {n*100}% dropout rate:\", results[model_name].history['val_loss'][-1])\n",
    "\n",
    "\n",
    "\n",
    "#plotting the val_loss for different neurons \n",
    "plt.plot(list(models_dropouts.values()), [results[model_name].history['val_loss'][-1] for model_name in results.keys()], 'ro')\n",
    "plt.ylabel(\"Val_loss\")\n",
    "plt.xlabel(\"dropout rate\")\n",
    "plt.title(\"Performance of model for different dropout rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently dropout doesn't help the model. It seems like it doesn't have much influence, because the order of magnitude of the validation losses is the same for every model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search was tried but unfortunately didn't work due to the non sufficient working memory in the computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "batch_size = [128, 256]\n",
    "#learn_rates = [1e-7, 1e-6, 1e-5, 1e-4, 0.001, 0.01, 0.1]\n",
    "activations = ['relu', 'tanh']\n",
    "grid_neurons = [2, 10, 25, 50]\n",
    "      \n",
    "param_grid = {'activation':activations,\n",
    "             'neurons': grid_neurons,\n",
    "             'batch_size': batch_size}\n",
    "    \n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "#https://www.adriangb.com/scikeras/stable/generated/scikeras.wrappers.KerasClassifier.html#scikeras.wrappers.KerasClassifier\n",
    "model = KerasRegressor(build_fn=create_model, epochs=50,  verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_gridsearch = GridSearchCV(estimator=model, param_grid=param_grid, scoring=None, n_jobs=-1, \n",
    "                                refit=True, cv=None, pre_dispatch = '1*n_jobs',verbose=2,\n",
    "                                return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed: 12.6min finished\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\disk.py:122: UserWarning: Unable to delete folder C:\\Users\\CORINN~1.WEG\\AppData\\Local\\Temp\\joblib_memmapping_folder_12612_6894980160 after 5 tentatives.\n",
      "  .format(folder_path, RM_SUBDIRS_N_RETRY))\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\CORINN~1.WEG\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_12612_6894980160\\\\12612-1751341378920-8ee34330cf3b47b183764aceafcbc26c.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-23e13bcb0121>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_gridsearch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_terminate_backend\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;31m# in latter calls but we free as much memory as we can by deleting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[1;31m# the shared memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdelete_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\disk.py\u001b[0m in \u001b[0;36mdelete_folder\u001b[1;34m(folder_path, onerror)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWindowsError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    511\u001b[0m             \u001b[1;31m# can't continue even if onerror hook returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    395\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\CORINN~1.WEG\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_12612_6894980160\\\\12612-1751341378920-8ee34330cf3b47b183764aceafcbc26c.pkl'"
     ]
    }
   ],
   "source": [
    "results = model_gridsearch.fit(train,train,epochs=50,validation_split=0.1, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
